from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.selector import Selector
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy import log, signals
from scrapy.xlib.pydispatch import dispatcher
import string
from datetime import datetime
import sys
sys.path.append('/home/ec2-user/bblio/build/')


class response:
    def __init__(self, body, url, encoding):
        self.body = body
        self.url = url
        self.encoding = encoding

this_sgml = SgmlLinkExtractor(unique=True)

r = response('<a href="www.test.com">link</a><a href="test/">test</a>','http://www.hello.com', 'utf-8')

print(this_sgml.extract_links(r))



class SpiderAll(CrawlSpider):
    name = None
    allowed_domains = None
    rules = None
    groupName = None
    count = 0
    id = None
    def __init__(self, *a, **kw):
        self.allowed_domains = kw['source_allowed_domains'].split(';')
        self.start_urls = kw['source_start_urls'].split(';')
        allowFollow = kw['source_allowFollow'].split(";")   
        denyFollow = None
        if kw['source_denyFollow']: 
            denyFollow = kw['source_denyFollow'].split(";")    
        
        denyParse = None
        allowParse = kw['source_allowParse'].split(";")
        if kw['source_denyParse']:
            denyParse = kw['source_denyParse'].split(";")
        
        self.rules = (
	Rule(SgmlLinkExtractor(allow=allowParse,deny=denyParse, unique=True,restrict_xpaths=('//*[not(self::meta)]')), 
        callback='parse_start_url', follow='true'),
        
        Rule(SgmlLinkExtractor(allow=allowFollow,deny=denyFollow,unique=True), follow='true'),
 
        )    
        super(SpiderAll, self).__init__(*a, **kw) 
        
    def parse_start_url(self, response):
        try:
            sel = Selector(response)
            sites = sel.xpath("//p|//li|//td|//div")
            #sites = sel.xpath("//")
            items = []
            item = URLItem()
            item['title'] = sel.xpath("//title/text()").extract()
            item['document_text'] = str(sites.xpath('text()').extract()).encode('utf8')
            item['urlAddress'] = response.url
            item['domain'] = self.allowed_domains
            item['site'] = Site.objects.get(pk=self.id)
            items.append(item)
        except AttributeError:
	    log.msg('* Cannot parse: ' + response.url,level=log.INFO)
            log.msg(sys.exc_info()[0], level=log.INFO)
            return
	else:
            self.count += 1
            if self.count == 10:
                log.msg('Scraped 10 items', level=log.INFO)
            return items

